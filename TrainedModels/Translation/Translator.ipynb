{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPRzgmAaYXdeOjMAg5qZKG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaskoudisDimi/Computational-Intelligence-and-Statistical-Learning/blob/master/TrainedModels/Translation/Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Theory\n",
        "# The goal of this class of models is to map a string input of a fixed-length to a paired string output of fixed length,\n",
        "# in which these two lengths can differ. If a string in the input language has 8 words, and the same sentence\n",
        "# in the target language has 4 words, then a high quality translator should infer that and shorten the sentence length of the output\n",
        "\n",
        "\n",
        "# Seq2Seq translators typically share a common framework. The three primary components of any Seq2Seq translator\n",
        "# are the encoder and decoder networks and an intermediary vector encoding between them.\n",
        "\n",
        "\n",
        "\n",
        "# The encoder network is a series of these RNN units. It uses these to sequentially encode the elements from\n",
        "# the input for the encoder vector, with the final hidden state being written to the intermediary vector.\n",
        "\n",
        "\n",
        "# Attention is the practice of forcing the decoder to focus on certain parts of the encoder's outputs\n",
        "# through a set of weights. These attention weights are multiplied by the encoder output vectors.\n",
        "# This produces a combined vector encoding that will augment the ability of the decoder to understand\n",
        "# the context of the outputs it is generating, and therefore improve its predictions.\n",
        "# Calculating these attention weights is done through a feed forward attention layer, which uses the decoders input\n",
        "# and hidden states as inputs.\n",
        "\n",
        "\n",
        "# The encoder vector contains the numerical representations of the input from the encoder. If things go correctly,\n",
        "# it captures all the information from the initial input sentence. This encoding vector then acts as the initial hidden\n",
        "# state for the decoder network.\n",
        "\n",
        "# The decoder network is essentially the inverse of the encoder. It takes the encoded vector intermediary as a hidden state,\n",
        "# and sequentially generates the translation Each element in the output informs the decoders prediction of the following element.\n",
        "\n",
        "\n",
        "\n",
        "# In practice, a NMT will take an input string of one language and creates a sequence of embeddings representing each element,\n",
        "# word, in the sentence. The RNN units in the encoders take both the previous hidden state and a single element of the original\n",
        "# input embedding as inputs, and each step can improve upon the previous step sequentially by accessing the hidden state\n",
        "# of the previous step to inform the predicted element. It is important to also mention that in addition to encoding the sentence,\n",
        "# an end of the sentence tag representation is included as an element in the sequence. This end of sentence tagging helps\n",
        "# the translator know what words in the translated language will trigger the decoder to quit decoding and output the translated\n",
        "# sentence.\n",
        "\n",
        "# The final hidden state embeddings are encoded in the intermediary encoder vector.\n",
        "# The encodings capture as much information as possible about the input sentence in order to facilitate the decoder\n",
        "# in decoding them into the translation. It can do this be virtue of being used as the initial hidden state\n",
        "# for the decoder network.\n",
        "\n",
        "# Using the information from the encoder vector, each recurrent unit in the decoder accepts a hidden state from the previous\n",
        "# unit and produces an output as well as its own hidden state. The decoder is informed by the hidden state to make a prediction\n",
        "# on a sequence, and with each sequential prediction, it predicts the next instance of the sequence using the information\n",
        "# from the previous hidden state. The final output is thus the end result of the step-wise predictions of each element\n",
        "# in the translated sentence. The length of this sentence is irrelevant to the input sentences length thanks to the end\n",
        "# of sentence tag, which tells the decoder when to stop adding terms to the sentence.\n",
        "\n",
        "\n",
        "\n",
        "# Encoder: The input sentence in the source language is passed through an encoder neural network, typically implemented\n",
        "# as a recurrent neural network (RNN) or a transformer model. The encoder processes the input sequence and produces\n",
        "# a fixed-size representation (context vector) that captures the information in the source language sentence.\n",
        "\n",
        "# Decoder: The context vector produced by the encoder is passed as the initial hidden state to the decoder network.\n",
        "# The decoder, which is another RNN or transformer, generates the output sentence in the target language. It does this\n",
        "# by producing one word at a time, conditioning each word generation on the previously generated words and the context vector.\n",
        "\n",
        "# Training: These models are trained on a parallel corpus of source and target language sentences. During training,\n",
        "# the model learns to minimize the difference between its predicted translations and the actual target translations.\n",
        "\n"
      ],
      "metadata": {
        "id": "ylZidTUWwF8J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!pip install -U torchdata\n",
        "!pip install portalocker\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "sldXuJXCwF_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "metadata": {
        "id": "UZvGAxdTwGHJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization:\n",
        "# The code begins by setting up tokenizers for the source and target languages. Tokenization is the process of breaking text into individual words or tokens. In this case, the code uses the spaCy tokenizers for the German and English languages.\n",
        "# token_transform[SRC_LANGUAGE] is set to a tokenizer for the German language ('de_core_news_sm'), and token_transform[TGT_LANGUAGE] is set to a tokenizer for the English language ('en_core_web_sm').\n",
        "\n",
        "# Helper Function for Yielding Tokens:\n",
        "# The code defines a helper function named yield_tokens which takes an iterable data iterator and a language as input.\n",
        "# Inside the function, it uses the previously defined tokenizers to tokenize the text samples from the data iterator for the specified language.\n",
        "# It yields the tokenized text as a list of strings.\n",
        "\n",
        "# Special Symbols and Indices:\n",
        "# The code defines special symbols and their corresponding indices. These symbols are often used in machine translation tasks:\n",
        "# UNK_IDX (Unknown Index): Used for tokens that are not in the vocabulary.\n",
        "# PAD_IDX (Padding Index): Used for padding sequences to the same length.\n",
        "# BOS_IDX (Beginning of Sentence Index): Marks the start of a sentence.\n",
        "# EOS_IDX (End of Sentence Index): Marks the end of a sentence.\n",
        "# These symbols are assigned integer indices, starting from 0 to 3.\n",
        "\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# Vocabulary Building:\n",
        "# The code iterates over the source and target languages, performing the following steps for each language:\n",
        "# It creates a training data iterator (train_iter) using the Multi30k dataset, which contains sentence pairs in the specified language pair (e.g., German and English).\n",
        "# It uses the build_vocab_from_iterator function from torchtext to build a vocabulary for the language.\n",
        "# The vocabulary is built from the tokenized text samples obtained using the yield_tokens function.\n",
        "# The min_freq=1 argument ensures that all tokens are included in the vocabulary, and specials=special_symbols includes the special symbols defined earlier in the vocabulary.\n",
        "# special_first=True ensures that the special symbols are placed at the beginning of the vocabulary.\n",
        "# Set Default Index:\n",
        "# After building the vocabulary for each language, the code sets the UNK_IDX as the default index for each vocabulary. This means that if a token is not found in the vocabulary, the index 0 (UNK_IDX) will be used by default.\n",
        "# This code snippet is an essential part of preparing text data for machine translation tasks, ensuring that text is tokenized, and vocabularies are built for both the source and target languages, with special symbols appropriately configured. These tokenized data and vocabularies can be used to train and evaluate machine translation models effectively.\n",
        "\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "CshGbVmPwGKC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Positional Encoding:\n",
        "# The PositionalEncoding class is defined as a helper module. It adds positional encoding to the token embeddings\n",
        "# to provide information about the word order in the input sequences.\n",
        "# Positional encoding is used to handle the sequence order since the Transformer architecture does not inherently consider\n",
        "# order. It helps the model understand the position of words in a sequence.\n",
        "# The class uses trigonometric functions to create a sinusoidal positional embedding for each position in the input sequence.\n",
        "# Token Embedding:\n",
        "# The TokenEmbedding class is another helper module. It converts input indices into token embeddings.\n",
        "# It uses an embedding layer to map token indices to continuous vector representations. The math.sqrt(self.emb_size) scaling\n",
        "# factor is applied to the embeddings to ensure they have the correct scale.\n",
        "# Seq2Seq Transformer Network:\n",
        "# The Seq2SeqTransformer class defines the main architecture of the sequence-to-sequence Transformer model.\n",
        "# It takes several hyperparameters as input, such as the number of encoder and decoder layers, embedding size (emb_size),\n",
        "# number of attention heads (nhead), vocabulary sizes for the source and target languages, and other settings.\n",
        "# The class consists of the following components:\n",
        "# Transformer: The core Transformer model is created with the specified configuration.\n",
        "# generator: A linear layer used to generate the output sequence.\n",
        "# src_tok_emb and tgt_tok_emb: Token embedding layers for the source and target languages.\n",
        "# positional_encoding: The positional encoding module is used to add positional information to the token embeddings.\n",
        "# The forward method takes the source and target sequences, source and target masks, and padding masks as inputs and\n",
        "# performs the forward pass of the model.\n",
        "# It returns the output from the Transformer model, which is then passed through the generator to obtain the final output.\n",
        "\n",
        "# Encoder and Decoder Functions:\n",
        "# The encode and decode methods are used to separately encode the source sequence and decode the target sequence.\n",
        "# The encoder encodes the source sequence, and the decoder generates the target sequence using the memory from the encoder.\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ],
      "metadata": {
        "id": "44s9fiuR4IpQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code defines two functions for creating masks used in the context of sequence-to-sequence models,\n",
        "# specifically for the Seq2Seq Transformer model previously described. These masks are essential for controlling\n",
        "# attention mechanisms and handling padding in the model. Let's break down the code:\n",
        "# generate_square_subsequent_mask(sz):\n",
        "\n",
        "# This function generates a square mask of size sz x sz. It's often used in self-attention mechanisms to prevent tokens\n",
        "# from attending to future tokens.\n",
        "# torch.triu creates an upper triangular matrix of ones and then compares it to 1, resulting in a binary matrix with ones\n",
        "# in the upper triangle and zeros elsewhere.\n",
        "# It is then transposed to swap rows and columns.\n",
        "# The mask is converted to a float tensor where zeros are filled with negative infinity (float('-inf')) and ones are filled\n",
        "# with 0.0.\n",
        "# This mask is used to ensure that during self-attention, tokens can only attend to previous or current tokens and not\n",
        "# to future tokens.\n",
        "# create_mask(src, tgt):\n",
        "\n",
        "# This function is responsible for creating several masks that will be used during the training and inference\n",
        "# phases of the Seq2Seq Transformer model.\n",
        "\n",
        "# It takes source (src) and target (tgt) sequences as input.\n",
        "\n",
        "# src_seq_len and tgt_seq_len represent the lengths of the source and target sequences, respectively.\n",
        "\n",
        "# tgt_mask:\n",
        "\n",
        "# Calls generate_square_subsequent_mask with the length of the target sequence (tgt_seq_len) to create a mask for the\n",
        "# target side. This ensures that during self-attention in the decoder, tokens cannot attend to future tokens.\n",
        "# src_mask:\n",
        "\n",
        "# Creates a square mask for the source side. However, this mask is filled with zeros. In this case, it's essentially\n",
        "# an identity matrix. This is because, in the encoder, each token can attend to all other tokens in the source sequence\n",
        "# without any restrictions.\n",
        "# src_padding_mask:\n",
        "\n",
        "# This mask is created to identify padding tokens in the source sequence. It checks if the values in the source sequence\n",
        "# are equal to a padding index (PAD_IDX) and transposes the result to make it suitable for use in the Transformer.\n",
        "# tgt_padding_mask:\n",
        "\n",
        "# Similar to the source padding mask, this one identifies padding tokens in the target sequence.\n",
        "# The function returns these masks, which will be used in the forward pass of the Seq2Seq Transformer model. They help\n",
        "# control which parts of the sequences the model pays attention to and how it handles padding tokens during processing.\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
        "\n",
        "\n",
        "# Setting a Random Seed:\n",
        "# It sets a random seed for reproducibility. This ensures that if you run the code multiple times,\n",
        "# you'll get the same results, which can be useful for debugging and experimentation.\n",
        "# Defining Model Hyperparameters:\n",
        "# Several hyperparameters for the model are defined:\n",
        "# SRC_VOCAB_SIZE and TGT_VOCAB_SIZE: The sizes of the source and target vocabularies, respectively.\n",
        "# These values are determined based on the vocabularies created earlier.\n",
        "# EMB_SIZE: The embedding size for tokens in the model (e.g., word embeddings).\n",
        "# NHEAD: The number of attention heads in the multi-head self-attention mechanism.\n",
        "# FFN_HID_DIM: The dimension of the feedforward neural network hidden layer within the Transformer.\n",
        "# BATCH_SIZE: The size of each batch of training data.\n",
        "# NUM_ENCODER_LAYERS and NUM_DECODER_LAYERS: The number of encoder and decoder layers in the Transformer model.\n",
        "# Initializing the Transformer Model:\n",
        "# A Seq2SeqTransformer model is created with the specified hyperparameters. This model will be used for the machine\n",
        "# translation task.\n",
        "# Weight Initialization:\n",
        "# The code initializes the model's weights. For weights with a dimension greater than 1, it uses Xavier (Glorot)\n",
        "# weight initialization. Weight initialization helps in training neural networks effectively.\n",
        "# Moving the Model to the Device (GPU or CPU):\n",
        "# The model is moved to the computing device specified earlier (DEVICE). If a CUDA-compatible GPU is available,\n",
        "# the model is moved to the GPU; otherwise, it runs on the CPU.\n",
        "# Loss Function:\n",
        "# The loss function is defined as cross-entropy loss. This is a common choice for sequence-to-sequence tasks,\n",
        "# where the goal is to minimize the difference between the predicted translations and the actual target translations.\n",
        "# The ignore_index parameter is set to PAD_IDX to ignore padding tokens during loss computation.\n",
        "# Optimizer:\n",
        "# The code sets up an Adam optimizer to update the model's parameters during training. It uses a learning rate\n",
        "# of 0.0001 and sets other hyperparameters for Adam, such as the beta values a\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "print(\"SRC_VOCAB_SIZE \", SRC_VOCAB_SIZE)\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "print(\"TGT_VOCAB_SIZE \", TGT_VOCAB_SIZE)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "\n",
        "# This is a higher-order function that takes a variable number of transformation functions as its arguments.\n",
        "# It returns a new function (func) that applies the provided transformations sequentially to the input text.\n",
        "# This function is used to apply a sequence of text transformations to a raw input text.\n",
        "# tensor_transform(token_ids: List[int]):\n",
        "# This function takes a list of token IDs (integers) as input.\n",
        "# It adds a beginning-of-sequence (BOS) token ID at the beginning, appends the token IDs from the input, and then\n",
        "# adds an end-of-sequence (EOS) token ID at the end.\n",
        "# The result is a tensor that represents the input sequence with BOS and EOS tokens.\n",
        "# text_transform:\n",
        "# This dictionary holds transformation functions for both the source (SRC_LANGUAGE) and target (TGT_LANGUAGE) languages.\n",
        "# These transformations include tokenization, numericalization, and the addition of BOS and EOS tokens.\n",
        "# collate_fn(batch):\n",
        "# This function is used to collate data samples into batch tensors.\n",
        "# It takes a batch of data samples, each consisting of a source and target text.\n",
        "# It applies the text_transform functions to each sample to tokenize, convert to tensors, and add BOS/EOS tokens.\n",
        "# The resulting source and target tensors are then padded to the same length using pad_sequence from torch.nn.utils.rnn.\n",
        "# Padding is done with the padding value PAD_IDX.\n",
        "# The function returns the source and target tensors, which can be used for training the Seq2Seq model.\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "\n",
        "\n",
        "# train_epoch(model, optimizer):\n",
        "# This function is responsible for training one epoch of the machine translation model.\n",
        "# It takes two main arguments: the model to be trained (model) and the optimizer that will update the model's parameters (optimizer).\n",
        "# The function follows these steps:\n",
        "# Sets the model in training mode using model.train().\n",
        "# Initializes the losses variable to keep track of the cumulative training loss.\n",
        "# Loads the training data using the Multi30k dataset with the specified source and target languages.\n",
        "# Creates a data loader (train_dataloader) for batching the data using the collate_fn function.\n",
        "# Iterates over batches of data from the data loader.\n",
        "# For each batch, it:\n",
        "# Moves the source (src) and target (tgt) tensors to the computing device specified earlier (DEVICE).\n",
        "# Prepares the target input by removing the last token from the target sequence. This is because the model's goal is to predict the next token given the previous tokens.\n",
        "# Generates masks using the create_mask function. These masks include source mask, target mask, source padding mask, and target padding mask.\n",
        "# Passes the source, target input, and masks to the model to obtain predictions (logits).\n",
        "# Initializes the optimizer's gradients to zero with optimizer.zero_grad().\n",
        "# Computes the loss using cross-entropy loss between the model's predictions and the target output (shifted by one token). The loss is reshaped to be suitable for the loss function.\n",
        "# Backpropagates the loss and updates the model's parameters using loss.backward() and optimizer.step(), respectively.\n",
        "# Adds the current batch's loss to the cumulative losses.\n",
        "# Finally, the function returns the average training loss over all batches in the epoch.\n",
        "# evaluate(model):\n",
        "\n",
        "# This function is used for evaluating the model's performance on a validation dataset.\n",
        "# It takes the trained model (model) as input and follows a similar structure to the training function:\n",
        "# Sets the model in evaluation mode using model.eval().\n",
        "# Initializes the losses variable to keep track of the cumulative validation loss.\n",
        "# Loads the validation data using the Multi30k dataset with the specified source and target languages.\n",
        "# Creates a data loader (val_dataloader) for batching the validation data using the collate_fn function.\n",
        "# Iterates over batches of data from the validation data loader.\n",
        "# For each batch, it performs the following steps:\n",
        "# Moves the source (src) and target (tgt) tensors to the computing device (DEVICE).\n",
        "# Prepares the target input in the same way as in the training function.\n",
        "# Generates masks using the create_mask function.\n",
        "# Passes the source, target input, and masks to the model to obtain predictions (logits).\n",
        "# Computes the loss between the model's predictions and the actual target output.\n",
        "# Adds the current batch's loss to the cumulative losses.\n",
        "# The function returns the average validation loss over all batches in the validation dataset.\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))\n",
        "\n",
        "# Training Loop:\n",
        "# This section contains a training loop that runs for a specified number of epochs (in this case, NUM_EPOCHS is set to 18).\n",
        "# In each epoch, the model is trained using the train_epoch function, and the training loss is recorded.\n",
        "# After training the model for an epoch, the evaluate function is used to calculate the validation loss.\n",
        "# The results, including the training and validation losses, are printed for each epoch, along with the time taken for that epoch.\n",
        "# Greedy Decoding for Inference:\n",
        "# The code defines two functions for generating translations during inference:\n",
        "# greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "# This function performs greedy decoding to generate an output sequence in the target language.\n",
        "# It takes the trained model (model), a source sequence (src), its associated source mask (src_mask), the maximum length of the output sequence (max_len), and the start symbol (start_symbol) as inputs.\n",
        "# It iteratively predicts the next token in the target sequence and appends it to the output until it reaches the maximum length or encounters an end-of-sequence token.\n",
        "# translate(model, src_sentence):\n",
        "# This function takes a trained model (model) and a source sentence in the source language (src_sentence) as inputs.\n",
        "# It tokenizes and processes the source sentence and then uses greedy_decode to generate the translation.\n",
        "# The generated translation is returned as a string.\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        ""
      ],
      "metadata": {
        "id": "nERFFaqd4IvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GgIqFMx52Di",
        "outputId": "12539743-f233-4d35-a01c-cfac48fc5389"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " A group of people standing in front of an igloo \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(transformer.state_dict(), 'translate_model.pth')"
      ],
      "metadata": {
        "id": "waSAVkLj4IyW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5IiAPfyd4I1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}